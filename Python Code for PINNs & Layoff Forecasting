import time, math
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_percentage_error

import torch
import torch.nn as nn
import torch.optim as optim

# ---------------------------
# Device & dtype
# ---------------------------
torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

EPS = 1e-10

# =====================================================
# DATA: layoffs annual 2016–2021
# =====================================================
years = [
    "2016","2017","2018","2019",
    "2020","2021","2022"
]

E_raw = np.array([
    21200000,21600000,21800000,
    21800000,41700000,17000000,
    17600000
], dtype=float)

N = len(E_raw)
t_np = np.linspace(0.0, 1.0, N)
t_tensor = torch.tensor(t_np.reshape(-1, 1), dtype=torch.float64, device=device)

# normalize layoffs
E_min, E_max = float(E_raw.min()), float(E_raw.max())
E_norm = (E_raw - E_min) / (E_max - E_min)
E_obs = torch.tensor(E_norm.reshape(-1, 1), dtype=torch.float64, device=device)


# =====================================================
# Fractional Caputo (Grünwald–Letnikov)
# =====================================================
def frac_binomial_coeffs(alpha, nmax):
    a_safe = torch.clamp(alpha, 0.01, 1.99)
    j = torch.arange(0, nmax + 1, dtype=torch.float64, device=device)
    num = torch.lgamma(a_safe + 1.0)
    denom = torch.lgamma(j + 1.0) + torch.lgamma(a_safe - j + 1.0)
    comb = torch.exp(num - denom)
    return (-1.0)**j * comb


def caputo_GL_derivative(u_vals, alpha):
    N = u_vals.shape[0]
    if N <= 1:
        return torch.zeros_like(u_vals)

    coeffs = frac_binomial_coeffs(alpha, N - 1)
    d = torch.zeros_like(u_vals)
    h_alpha = (1.0 + EPS) ** alpha

    for n in range(1, N):
        seg = u_vals[:n+1].flip(0)
        val = coeffs[:n+1] @ seg
        d[n] = val / (h_alpha + EPS)

    return d


# =====================================================
# Neural PINN model
# =====================================================
class PINN_NN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 4),
        )

        self.state_scale = {"A":1,"C":1.5,"E":1,"Ep":1}

        # === Base parameters (will never deviate > 0.01) ===
        self.base_vals = {
            'sigma':0.3737,'gamma':0.112404,'muA':0.127412,'Lambda':2.97,'betaC':0.7812,'muC':0.5436,
            's':0.886556,'zeta':0.1684,'eta':0.28056,'muE':0.3518,'kappa':0.1310,'mu0':0.18156,
            'alpha':0.74,'lam':2.0
        }

        # raw parameters for tanh-mapping
        self.param_raw = nn.ParameterDict({
            k: nn.Parameter(torch.zeros(1, dtype=torch.float64))
            for k in self.base_vals.keys()
        })

        self.apply(self._init_weights)

    @staticmethod
    def _init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            nn.init.zeros_(m.bias)

    # ---------------------------
    # Bounded states
    # ---------------------------
    def forward(self, t):
        x = self.net(t)
        A = torch.sigmoid(x[:,0:1])*self.state_scale["A"]
        C = torch.sigmoid(x[:,1:2])*self.state_scale["C"]
        E = torch.sigmoid(x[:,2:3])*self.state_scale["E"]
        Ep= torch.sigmoid(x[:,3:4])*self.state_scale["Ep"]
        return A,C,E,Ep

    # ---------------------------
    # Parameter bounds ≤ ±0.0001
    # ---------------------------
    def constrained_params(self):
        dev = 0.0001
        p = {}
        for k,v in self.base_vals.items():
            p[k] = v + dev * torch.tanh(self.param_raw[k])
        return p


# =====================================================
# RHS
# =====================================================
def rhs(A,C,E,Ep,p):
    return (
        p['sigma']*A*C - p['gamma']*A*E - p['muA']*A,
        p['Lambda'] - p['betaC']*A*C - p['muC']*C,
        p['s']*C + p['zeta']*A*C - p['eta']*E - p['muE']*E,
        p['eta']*E + p['kappa']*C - p['mu0']*Ep
    )


# =====================================================
# Training
# =====================================================
model = PINN_NN().to(device)
opt = optim.Adam(model.parameters(), lr=3e-4)

max_epochs = 20000
w_phys = 1
w_data = 40
w_ic = 200
print_every = 1000

loss_hist = []

A0,C0,E0,Ep0 = 0.01,0.8,E_norm[0],0

best_loss = 1e18
best_state = None
patience = 1000
wait = 0

for ep in range(1,max_epochs+1):
    opt.zero_grad()

    A,C,E,Ep = model(t_tensor)
    p = model.constrained_params()

    dA = caputo_GL_derivative(A.squeeze(), p['alpha'])
    dC = caputo_GL_derivative(C.squeeze(), p['alpha'])
    dE = caputo_GL_derivative(E.squeeze(), p['alpha'])
    dEp= caputo_GL_derivative(Ep.squeeze(), p['alpha'])

    lam = p['lam']**(p['alpha']-1)

    rA,rC,rE,rEp = rhs(A.squeeze(),C.squeeze(),E.squeeze(),Ep.squeeze(),p)

    phys = (
        (lam*dA-rA)[1:].pow(2).mean() +
        (lam*dC-rC)[1:].pow(2).mean() +
        (lam*dE-rE)[1:].pow(2).mean() +
        (lam*dEp-rEp)[1:].pow(2).mean()
    )
    ic = (A[0]-A0).pow(2)+(C[0]-C0).pow(2)+(E[0]-E0).pow(2)+(Ep[0]-Ep0).pow(2)
    data = (E-E_obs).pow(2).mean()

    loss = w_phys*phys + w_data*data + w_ic*ic
    loss.backward()
    opt.step()

    loss_hist.append(float(loss))

    if loss < best_loss:
        best_loss = loss
        best_state = {k:v.cpu() for k,v in model.state_dict().items()}
        wait = 0
    else:
        wait+=1
        if wait>patience:
            print("Early stopping.")
            break

    if ep % print_every == 0:
        print(f"Epoch {ep} | Loss={float(loss):.4e}")

model.load_state_dict(best_state)


# =====================================================
# Parameter output
# =====================================================
p = model.constrained_params()
print("\n===== ESTIMATED PARAMETERS (Within ±0.01) =====")
for k,v in p.items():
    print(f"{k:10s} → {float(v):.6f}")
print("================================================\n")


# =====================================================
# Model Fit
# =====================================================
with torch.no_grad():
    _,_,E_fit,_ = model(t_tensor)

E_fit = E_fit.squeeze().cpu().numpy()
E_denorm = E_fit*(E_max-E_min)+E_min

mape = mean_absolute_percentage_error(E_raw,E_denorm)*100
print(f"Training MAPE: {mape:.2f}%\n")


# =====================================================
# Forecast Only 2023–2024 (2 years)
# =====================================================
future_points = 2
t_future = np.linspace(1+1/(N-1), 1 + future_points/(N-1), future_points)
t_future = torch.tensor(t_future.reshape(-1,1), dtype=torch.float64, device=device)

with torch.no_grad():
    _,_,E_future,_ = model(t_future)

E_future = E_future.squeeze().cpu().numpy()
E_future_denorm = E_future*(E_max - E_min)+E_min

last_year = int(years[-1])
forecast_years = [str(last_year+i) for i in range(1,future_points+1)]

print("========== FORECAST ==========")
for yr,val in zip(forecast_years,E_future_denorm):
    print(f"{yr}  → {val:,.0f}")
print("==============================\n")


# =====================================================
# Plot loss curve
# =====================================================
plt.figure(figsize=(6,3))
plt.plot(loss_hist)
plt.grid(True)
plt.title("Training Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()


# =====================================================
# Plot Fit + Forecast
# =====================================================
plt.figure(figsize=(8,4))
plt.plot(years, E_raw, 'ko-', label="Observed")
plt.plot(years, E_denorm, 'b--', label="PINN Fit")
plt.plot(forecast_years, E_future_denorm, 'g^-', label="2023–2024 Forecast")
plt.grid(True)
plt.ylabel("Layoffs")
plt.title("Automation–Layoffs PINN Forecast")
plt.legend()
plt.show()
